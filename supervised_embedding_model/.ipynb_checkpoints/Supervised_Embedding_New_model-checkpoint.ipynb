{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smajumdar/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from make_tensor import make_tensor, load_vocab\n",
    "from sklearn import metrics\n",
    "from sys import argv\n",
    "from test import evaluate\n",
    "from utils import batch_iter, neg_sampling_iter, new_neg_sampling_iter\n",
    "from data_utils import *\n",
    "from itertools import chain\n",
    "from six.moves import range, reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = 'data/train-task-1.tsv'\n",
    "dev = 'data/dev-task-1-500.tsv'\n",
    "vocab_file = 'data/vocab-task-1.tsv'\n",
    "candidates = 'data/candidates-1.tsv'\n",
    "embedding_size = 20\n",
    "save_dir = 'checkpoints/task-1/model'\n",
    "margin = 0.01\n",
    "negative_cand = 100\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object) :\n",
    "    def __init__(self,vocab_dim=1000,embedding_size=20,margin=0.01,learning_rate=1e-2,batch_size=32,epochs=20) :\n",
    "        self.vocab_dim = vocab_dim\n",
    "        self.embedding_size = embedding_size\n",
    "        self.random_seed = 42\n",
    "        self.margin = margin\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = 32\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        self.session = tf.Session()\n",
    "        \n",
    "        self.config = tf.ConfigProto()\n",
    "        self.config.gpu_options.allow_growth = True\n",
    "        self.build_model()\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        \n",
    "        init_op = tf.initialize_all_variables()\n",
    "        self.session.run(init_op)\n",
    "        \n",
    "    def build_model(self) :\n",
    "        self.create_variables()\n",
    "        tf.set_random_seed(self.random_seed + 1)\n",
    "        \n",
    "        self.A_var = tf.Variable(initial_value=tf.random_uniform(shape=[self.embedding_size,self.vocab_dim],minval=-1,maxval=1,seed=(self.random_seed + 2)))\n",
    "        self.B_var = tf.Variable(initial_value=tf.random_uniform(shape=[self.embedding_size,self.vocab_dim],minval=-1,maxval=1,seed=(self.random_seed + 2)))\n",
    "        \n",
    "        self.global_step = tf.Variable(0,dtype=tf.int32,trainable=False,name='global_step')\n",
    "        \n",
    "        self.cont_mult = tf.transpose(tf.matmul(self.A_var,tf.transpose(self.context_batch)))\n",
    "        self.resp_mult = tf.matmul(self.B_var,tf.transpose(self.response_batch))\n",
    "        self.neg_resp_mult = tf.matmul(self.B_var,tf.transpose(self.neg_response_batch))\n",
    "        \n",
    "        self.pos_raw_f = tf.diag_part(tf.matmul(self.cont_mult,self.resp_mult))\n",
    "        self.neg_raw_f = tf.diag_part(tf.matmul(self.cont_mult,self.neg_resp_mult))\n",
    "        \n",
    "        self.f_pos = self.pos_raw_f\n",
    "        self.f_neg =self. neg_raw_f\n",
    "        \n",
    "        self.predict_op = tf.argmax(tf.nn.relu(self.f_neg - self.f_pos + self.margin),1,name='predict_op')\n",
    "        \n",
    "        self.loss = tf.reduce_sum(tf.nn.relu(self.f_neg - self.f_pos + self.margin))\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "    def create_variables(self) :\n",
    "        \n",
    "        self.context_batch = tf.placeholder(dtype=tf.float32,name='Context',shape=[None,self.vocab_dim])\n",
    "        self.response_batch = tf.placeholder(dtype=tf.float32,name='Response',shape=[None,self.vocab_dim])\n",
    "        self.neg_response_batch = tf.placeholder(dtype=tf.float32,name='NegResponse',shape=[None,self.vocab_dim])\n",
    "        \n",
    "    def _init_summaries(self) :\n",
    "        self.accuracy = tf.placeholder_with_default(0.0,shape=(),name='Accuracy')\n",
    "        self.accuracy_summary = tf.scaler_summary('Accuracy summary',self.accuracy)\n",
    "        \n",
    "        self.f_pos_summary = tf.histogram_summary('f_pos',self.f_pos)\n",
    "        self.f_neg_summary = tf.histogram_summary('f_neg',self.f_neg)\n",
    "        \n",
    "        self.loss_summary = tf.scaler_summary('Mini-batch loss',self.loss)\n",
    "        \n",
    "        self.summary_op = tf.merge_summary([self.f_pos_summary,self.f_neg_summary,self.loss_summary])\n",
    "        \n",
    "    def batch_fit(self,context_batch,response_batch,neg_response_batch) :\n",
    "        feed_dict = {self.context_batch : context_batch, self.response_batch : response_batch, self.neg_response_batch : neg_response_batch}\n",
    "        loss = self.session.run([self.loss,self.optimizer],feed_dict=feed_dict)\n",
    "        return loss\n",
    "    \n",
    "    def batch_compute_loss(self,context_batch,response_batch,neg_response_batch) :\n",
    "        feed_dict = {self.context_batch : context_batch, self.response_batch : response_batch, self.neg_response_batch : neg_response_batch}\n",
    "        loss = self.session.run([self.loss],feed_dict=feed_dict)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedEmbeddingModel(object) :\n",
    "    def __init__(self,vocab_dim,embedding_size,random_seed,margin,negative_cand,batch_size,epochs) :\n",
    "        self.vocab_dim = vocab_dim\n",
    "        self.embedding_size = embedding_size\n",
    "        self.random_seed = random_seed\n",
    "        self.margin = margin\n",
    "        self.negative_cand = negative_cand\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        \n",
    "        self.model = Model(vocab_dim=self.vocab_dim,embedding_size=self.embedding_size,margin=self.margin,batch_size=self.batch_size,epochs=self.epochs)\n",
    "        \n",
    "        \n",
    "    def train(self,train_context,train_response,train_candidates) :\n",
    "        \n",
    "        \n",
    "        n_train = len(train_context)\n",
    "        \n",
    "        train_batches = zip(range(0,n_train-self.batch_size,self.batch_size),range(self.batch_size,n_train,self.batch_size))\n",
    "        train_batches = [(start,end) for start,end in train_batches]\n",
    "        \n",
    "        for t in range(1,self.epochs + 1) :\n",
    "            print(\"Epoch# {}\".format(t))\n",
    "            np.random.shuffle(train_batches)\n",
    "            total_cost = 0.0\n",
    "            neg_samples = list()\n",
    "            for start, end in tqdm(train_batches) :\n",
    "                \n",
    "                neg_train_response = np.array(train_response.tolist()[:max(0,start - 1)] + train_response.tolist()[min(n_train,end + 1):])\n",
    "                neg_samples = new_neg_sampling_iter(neg_train_response,self.batch_size,self.negative_cand)\n",
    "                for n in neg_samples :\n",
    "                    c = train_context[start:end]\n",
    "                    r = train_response[start:end]\n",
    "                    loss = self.model.batch_fit(c,r,n)\n",
    "                    total_cost += loss[0]\n",
    "            total_cost /= (n_train*self.negative_cand)\n",
    "            print('Train cost : {}'.format(str(total_cost)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedEmbeddingModel2(object) :\n",
    "    def __init__(self,embedding_size,random_seed,margin,negative_cand,batch_size,epochs,data_dir) :\n",
    "        \n",
    "        self.embedding_size = embedding_size\n",
    "        self.random_seed = random_seed\n",
    "        self.margin = margin\n",
    "        self.negative_cand = negative_cand\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.data_dir = data_dir\n",
    "        self.OOV = False\n",
    "        candidates, self.candid2indx = load_candidates(self.data_dir)\n",
    "        self.n_cand = len(candidates)\n",
    "        print(\"Candidate Size\", self.n_cand)\n",
    "        self.indx2candid = dict((self.candid2indx[key], key) for key in self.candid2indx)\n",
    "        # task data\n",
    "        self.trainData, self.testData, self.valData = load_dialog_task(self.data_dir, self.candid2indx, self.OOV)\n",
    "        data = self.trainData + self.testData + self.valData\n",
    "        self.build_vocab(data, candidates)\n",
    "        # self.candidates_vec=vectorize_candidates_sparse(candidates,self.word_idx)\n",
    "        self.candidates_vec = vectorize_candidates(candidates, self.word_idx, self.candidate_sentence_size)\n",
    "        \n",
    "        self.model = Model(vocab_dim=self.vocab_size,embedding_size=self.embedding_size,margin=self.margin,batch_size=self.batch_size,epochs=self.epochs)\n",
    "        \n",
    "    def build_vocab(self, data, candidates):\n",
    "        vocab = reduce(lambda x, y: x | y, (set(\n",
    "            list(chain.from_iterable(s)) + q) for s, q, a in data))\n",
    "        vocab |= reduce(lambda x, y: x | y, (set(candidate)\n",
    "                                             for candidate in candidates))\n",
    "        vocab = sorted(vocab)\n",
    "        self.word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "        max_story_size = max(map(len, (s for s, _, _ in data)))\n",
    "        mean_story_size = int(np.mean([len(s) for s, _, _ in data]))\n",
    "        self.sentence_size = max(\n",
    "            map(len, chain.from_iterable(s for s, _, _ in data)))\n",
    "        self.candidate_sentence_size = max(map(len, candidates))\n",
    "        query_size = max(map(len, (q for _, q, _ in data)))\n",
    "        \n",
    "        self.vocab_size = len(self.word_idx) + 1  # +1 for nil word\n",
    "        self.sentence_size = max(\n",
    "            query_size, self.sentence_size)  # for the position\n",
    "        # params\n",
    "        print(\"vocab size:\", self.vocab_size)\n",
    "        print(\"Longest sentence length\", self.sentence_size)\n",
    "        print(\"Longest candidate sentence length\",\n",
    "              self.candidate_sentence_size)\n",
    "        print(\"Longest story length\", max_story_size)\n",
    "        print(\"Average story length\", mean_story_size)\n",
    "        \n",
    "    def train(self) :\n",
    "        \n",
    "        self.train_file_name = os.path.join(self.data_dir,'train_data.txt')\n",
    "        self.val_file_name = os.path.join(self.data_dir,'val_data.txt')\n",
    "        self.candidate_file_name = os.path.join(self.data_dir,'candidates.txt')\n",
    "        \n",
    "        vocab = self.word_idx\n",
    "        \n",
    "        print(\"Name of train file is : {}\\n type of vocab is : {}\".format(self.train_file_name,type(vocab)))\n",
    "        #train_context, train_response = make_tensor(self.train_file_name,self.word_idx)\n",
    "        train_context, train_response = create_tensor(self.data_dir,'train_data.txt',self.word_idx)\n",
    "        print(\"length of train_context : {}, length of train_response : {}\".format(len(train_context),len(train_response)))\n",
    "        train_candidates, _ = make_tensor(candidate_file_name,vocab)\n",
    "\n",
    "        val_context,val_response = make_tensor(val_file_name,vocab)\n",
    "        n_train = len(train_context)\n",
    "        \n",
    "        train_batches = zip(range(0,n_train-self.batch_size,self.batch_size),range(self.batch_size,n_train,self.batch_size))\n",
    "        train_batches = [(start,end) for start,end in train_batches]\n",
    "        \n",
    "        for t in range(1,self.epochs + 1) :\n",
    "            print(\"Epoch# {}\".format(t))\n",
    "            np.random.shuffle(train_batches)\n",
    "            total_cost = 0.0\n",
    "            neg_samples = list()\n",
    "            for start, end in tqdm(train_batches) :\n",
    "                \n",
    "                neg_train_response = np.array(train_response.tolist()[:max(0,start - 1)] + train_response.tolist()[min(n_train,end + 1):])\n",
    "                neg_samples = new_neg_sampling_iter(neg_train_response,self.batch_size,self.negative_cand)\n",
    "                for n in neg_samples :\n",
    "                    c = train_context[start:end]\n",
    "                    r = train_response[start:end]\n",
    "                    loss = self.model.batch_fit(c,r,n)\n",
    "                    total_cost += loss[0]\n",
    "            total_cost /= (n_train*self.negative_cand)\n",
    "            print('Train cost : {}'.format(str(total_cost)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tensor(file_directory,file_name,vocab_dict) :\n",
    "    context_response_pairs = list()\n",
    "    f_handle = open(os.path.join(file_directory,file_name))\n",
    "    for line in f_handle :\n",
    "        sentences = line[1:].strip().split('\\t')\n",
    "        if len(sentences) != 2 :\n",
    "            context_response_pairs.append((sentences[0],'<silence>'))\n",
    "        else :\n",
    "            context_response_pairs.append((sentences[0],sentences[1]))\n",
    "    \n",
    "    f_handle.close()\n",
    "    context_list = list()\n",
    "    response_list = list()\n",
    "    for i, context_response in enumerate(context_response_pairs) :\n",
    "        context, response = context_response\n",
    "        \n",
    "        vec1 = np.zeros(len(vocab_dict)+1)\n",
    "        for w in context.split(' ') :\n",
    "            try :\n",
    "                vec1[vocab_dict[w]] = 1\n",
    "            except KeyError :\n",
    "                pass\n",
    "        vec2 = np.zeros(len(vocab_dict)+1)\n",
    "        \n",
    "        for w in response.split(' ') :\n",
    "            try :\n",
    "                vec2[vocab_dict[w]] = 1\n",
    "            except KeyError :\n",
    "                pass\n",
    "        context_list.append(vec1)\n",
    "        response_list.append(vec2)\n",
    "    context_tensor = np.array(context_list)\n",
    "    response_tensor = np.array(response_list)\n",
    "    return context_tensor, response_tensor\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smajumdar/anaconda3/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate Size 4212\n",
      "train_file is data/train_data.txt \n",
      "validation file is data/val_data.txt \n",
      "test file is data/test_data.txt \n",
      "vocab size: 3713\n",
      "Longest sentence length 20\n",
      "Longest candidate sentence length 9\n",
      "Longest story length 14\n",
      "Average story length 5\n",
      "Name of train file is : data/train_data.txt\n",
      " type of vocab is : <class 'dict'>\n",
      "length of train_context : 7024, length of train_response : 7024\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'candidate_file_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-08128b588a74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m                                   data_dir='data/')\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-118-44542015b912>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mtrain_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'train_data.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"length of train_context : {}, length of train_response : {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mtrain_candidates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_file_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mval_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_file_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'candidate_file_name' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "model = SupervisedEmbeddingModel2(embedding_size=embedding_size,\n",
    "                                 random_seed=42,\n",
    "                                 margin=margin,\n",
    "                                 negative_cand=negative_cand,\n",
    "                                 batch_size=batch_size,\n",
    "                                 epochs=epochs,\n",
    "                                  data_dir='data/')\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
