{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smajumdar/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from make_tensor import make_tensor, load_vocab, new_make_tensor\n",
    "from sklearn import metrics\n",
    "from sys import argv\n",
    "from test import evaluate\n",
    "from utils import batch_iter, neg_sampling_iter, new_neg_sampling_iter\n",
    "from data_utils import *\n",
    "from itertools import chain\n",
    "from six.moves import range, reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = 'data/train-task-1.tsv'\n",
    "dev = 'data/dev-task-1-500.tsv'\n",
    "vocab_file = 'data/vocab-task-1.tsv'\n",
    "candidates = 'data/candidates-1.tsv'\n",
    "embedding_size = 20\n",
    "save_dir = 'checkpoints/task-1/model'\n",
    "margin = 0.01\n",
    "negative_cand = 100\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object) :\n",
    "    def __init__(self,vocab_dim=1000,embedding_size=20,margin=0.01,learning_rate=1e-2,batch_size=32,epochs=20) :\n",
    "        self.vocab_dim = vocab_dim\n",
    "        self.embedding_size = embedding_size\n",
    "        self.random_seed = 42\n",
    "        self.margin = margin\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = 32\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        self.session = tf.Session()\n",
    "        \n",
    "        self.config = tf.ConfigProto()\n",
    "        self.config.gpu_options.allow_growth = True\n",
    "        self.build_model()\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "        \n",
    "        init_op = tf.initialize_all_variables()\n",
    "        self.session.run(init_op)\n",
    "        \n",
    "    def build_model(self) :\n",
    "        self.create_variables()\n",
    "        tf.set_random_seed(self.random_seed + 1)\n",
    "        \n",
    "        self.A_var = tf.Variable(initial_value=tf.random_uniform(shape=[self.embedding_size,self.vocab_dim],minval=-1,maxval=1,seed=(self.random_seed + 2)))\n",
    "        self.B_var = tf.Variable(initial_value=tf.random_uniform(shape=[self.embedding_size,self.vocab_dim],minval=-1,maxval=1,seed=(self.random_seed + 2)))\n",
    "        \n",
    "        self.global_step = tf.Variable(0,dtype=tf.int32,trainable=False,name='global_step')\n",
    "        \n",
    "        self.cont_mult = tf.transpose(tf.matmul(self.A_var,tf.transpose(self.context_batch)))\n",
    "        self.resp_mult = tf.matmul(self.B_var,tf.transpose(self.response_batch))\n",
    "        self.neg_resp_mult = tf.matmul(self.B_var,tf.transpose(self.neg_response_batch))\n",
    "        \n",
    "        self.pos_raw_f = tf.diag_part(tf.matmul(self.cont_mult,self.resp_mult))\n",
    "        self.neg_raw_f = tf.diag_part(tf.matmul(self.cont_mult,self.neg_resp_mult))\n",
    "        \n",
    "        self.f_pos = self.pos_raw_f\n",
    "        self.f_neg =self. neg_raw_f\n",
    "        \n",
    "        self.predict_op = tf.argmax(tf.nn.relu(self.f_neg - self.f_pos + self.margin),1,name='predict_op')\n",
    "        \n",
    "        self.loss = tf.reduce_sum(tf.nn.relu(self.f_neg - self.f_pos + self.margin))\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "    def create_variables(self) :\n",
    "        \n",
    "        self.context_batch = tf.placeholder(dtype=tf.float32,name='Context',shape=[None,self.vocab_dim])\n",
    "        self.response_batch = tf.placeholder(dtype=tf.float32,name='Response',shape=[None,self.vocab_dim])\n",
    "        self.neg_response_batch = tf.placeholder(dtype=tf.float32,name='NegResponse',shape=[None,self.vocab_dim])\n",
    "        \n",
    "    def _init_summaries(self) :\n",
    "        self.accuracy = tf.placeholder_with_default(0.0,shape=(),name='Accuracy')\n",
    "        self.accuracy_summary = tf.scaler_summary('Accuracy summary',self.accuracy)\n",
    "        \n",
    "        self.f_pos_summary = tf.histogram_summary('f_pos',self.f_pos)\n",
    "        self.f_neg_summary = tf.histogram_summary('f_neg',self.f_neg)\n",
    "        \n",
    "        self.loss_summary = tf.scaler_summary('Mini-batch loss',self.loss)\n",
    "        \n",
    "        self.summary_op = tf.merge_summary([self.f_pos_summary,self.f_neg_summary,self.loss_summary])\n",
    "        \n",
    "    def batch_fit(self,context_batch,response_batch,neg_response_batch) :\n",
    "        feed_dict = {self.context_batch : context_batch, self.response_batch : response_batch, self.neg_response_batch : neg_response_batch}\n",
    "        loss = self.session.run([self.loss,self.optimizer],feed_dict=feed_dict)\n",
    "        return loss\n",
    "    \n",
    "    def batch_compute_loss(self,context_batch,response_batch,neg_response_batch) :\n",
    "        feed_dict = {self.context_batch : context_batch, self.response_batch : response_batch, self.neg_response_batch : neg_response_batch}\n",
    "        loss = self.session.run([self.loss],feed_dict=feed_dict)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedEmbeddingModel(object) :\n",
    "    def __init__(self,vocab_dim,embedding_size,random_seed,margin,negative_cand,batch_size,epochs) :\n",
    "        self.vocab_dim = vocab_dim\n",
    "        self.embedding_size = embedding_size\n",
    "        self.random_seed = random_seed\n",
    "        self.margin = margin\n",
    "        self.negative_cand = negative_cand\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        \n",
    "        self.model = Model(vocab_dim=self.vocab_dim,embedding_size=self.embedding_size,margin=self.margin,batch_size=self.batch_size,epochs=self.epochs)\n",
    "        \n",
    "        \n",
    "    def train(self,train_context,train_response,train_candidates) :\n",
    "        \n",
    "        \n",
    "        n_train = len(train_context)\n",
    "        \n",
    "        train_batches = zip(range(0,n_train-self.batch_size,self.batch_size),range(self.batch_size,n_train,self.batch_size))\n",
    "        train_batches = [(start,end) for start,end in train_batches]\n",
    "        \n",
    "        for t in range(1,self.epochs + 1) :\n",
    "            print(\"Epoch# {}\".format(t))\n",
    "            np.random.shuffle(train_batches)\n",
    "            total_cost = 0.0\n",
    "            neg_samples = list()\n",
    "            for start, end in tqdm(train_batches) :\n",
    "                \n",
    "                neg_train_response = np.array(train_response.tolist()[:max(0,start - 1)] + train_response.tolist()[min(n_train,end + 1):])\n",
    "                neg_samples = new_neg_sampling_iter(neg_train_response,self.batch_size,self.negative_cand)\n",
    "                for n in neg_samples :\n",
    "                    c = train_context[start:end]\n",
    "                    r = train_response[start:end]\n",
    "                    loss = self.model.batch_fit(c,r,n)\n",
    "                    total_cost += loss[0]\n",
    "            total_cost /= (n_train*self.negative_cand)\n",
    "            print('Train cost : {}'.format(str(total_cost)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedEmbeddingModel2(object) :\n",
    "    def __init__(self,vocab_dim,embedding_size,random_seed,margin,negative_cand,batch_size,epochs,data_dir) :\n",
    "        self.vocab_dim = vocab_dim\n",
    "        self.embedding_size = embedding_size\n",
    "        self.random_seed = random_seed\n",
    "        self.margin = margin\n",
    "        self.negative_cand = negative_cand\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.data_dir = data_dir\n",
    "        self.OOV = False\n",
    "        candidates, self.candid2indx = load_candidates(self.data_dir)\n",
    "        self.n_cand = len(candidates)\n",
    "        print(\"Candidate Size\", self.n_cand)\n",
    "        self.indx2candid = dict((self.candid2indx[key], key) for key in self.candid2indx)\n",
    "        # task data\n",
    "        self.trainData, self.testData, self.valData = load_dialog_task(self.data_dir, self.candid2indx, self.OOV)\n",
    "        data = self.trainData + self.testData + self.valData\n",
    "        self.build_vocab(data, candidates)\n",
    "        # self.candidates_vec=vectorize_candidates_sparse(candidates,self.word_idx)\n",
    "        self.candidates_vec = vectorize_candidates(candidates, self.word_idx, self.candidate_sentence_size)\n",
    "        \n",
    "        self.model = Model(vocab_dim=self.vocab_dim,embedding_size=self.embedding_size,margin=self.margin,batch_size=self.batch_size,epochs=self.epochs)\n",
    "        \n",
    "    def build_vocab(self, data, candidates):\n",
    "        vocab = reduce(lambda x, y: x | y, (set(\n",
    "            list(chain.from_iterable(s)) + q) for s, q, a in data))\n",
    "        vocab |= reduce(lambda x, y: x | y, (set(candidate)\n",
    "                                             for candidate in candidates))\n",
    "        vocab = sorted(vocab)\n",
    "        self.word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "        max_story_size = max(map(len, (s for s, _, _ in data)))\n",
    "        mean_story_size = int(np.mean([len(s) for s, _, _ in data]))\n",
    "        self.sentence_size = max(\n",
    "            map(len, chain.from_iterable(s for s, _, _ in data)))\n",
    "        self.candidate_sentence_size = max(map(len, candidates))\n",
    "        query_size = max(map(len, (q for _, q, _ in data)))\n",
    "        \n",
    "        self.vocab_size = len(self.word_idx) + 1  # +1 for nil word\n",
    "        self.sentence_size = max(\n",
    "            query_size, self.sentence_size)  # for the position\n",
    "        # params\n",
    "        print(\"vocab size:\", self.vocab_size)\n",
    "        print(\"Longest sentence length\", self.sentence_size)\n",
    "        print(\"Longest candidate sentence length\",\n",
    "              self.candidate_sentence_size)\n",
    "        print(\"Longest story length\", max_story_size)\n",
    "        print(\"Average story length\", mean_story_size)\n",
    "        \n",
    "    def train(self,train_context,train_response,train_candidates) :\n",
    "        \n",
    "        train_file_name = os.path.join(self.data_dir,'train_data.txt')\n",
    "        val_file_name = os.path.join(self.data_dir,'val_data.txt')\n",
    "        candidate_file_name = os.path.join(self.data_dir,'candidates.txt')\n",
    "        \n",
    "        vocab = self.word_idx\n",
    "        \n",
    "        train_context, train_response = new_make_tensor(train_file_name,vocab)\n",
    "        print(\"length of train_context : {}, length of train_response : {}\".format(len(train_context),len(train_response)))\n",
    "        train_candidates, _ = new_make_tensor(candidate_file_name,vocab)\n",
    "\n",
    "        val_context,val_response = new_make_tensor(val_file_name,vocab)\n",
    "        n_train = len(train_context)\n",
    "        \n",
    "        train_batches = zip(range(0,n_train-self.batch_size,self.batch_size),range(self.batch_size,n_train,self.batch_size))\n",
    "        train_batches = [(start,end) for start,end in train_batches]\n",
    "        \n",
    "        for t in range(1,self.epochs + 1) :\n",
    "            print(\"Epoch# {}\".format(t))\n",
    "            np.random.shuffle(train_batches)\n",
    "            total_cost = 0.0\n",
    "            neg_samples = list()\n",
    "            for start, end in tqdm(train_batches) :\n",
    "                \n",
    "                neg_train_response = np.array(train_response.tolist()[:max(0,start - 1)] + train_response.tolist()[min(n_train,end + 1):])\n",
    "                neg_samples = new_neg_sampling_iter(neg_train_response,self.batch_size,self.negative_cand)\n",
    "                for n in neg_samples :\n",
    "                    c = train_context[start:end]\n",
    "                    r = train_response[start:end]\n",
    "                    loss = self.model.batch_fit(c,r,n)\n",
    "                    total_cost += loss[0]\n",
    "            total_cost /= (n_train*self.negative_cand)\n",
    "            print('Train cost : {}'.format(str(total_cost)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing_Vocab\n",
      "type of vocab : <class 'dict'>\n",
      "{'indian': 0, 'four': 1, 'am': 2, 'morning': 3, 'paris': 4, 'to': 5, 'what': 6, 'a': 7, 'we': 8, 'some': 9, 'may': 10, 'food': 11, 'me': 12, 'reservation': 13, 'six': 14, 'have': 15, 'moderate': 16, 'price': 17, 'good': 18, \"i'm\": 19, 'of': 20, 'french': 21, 'would': 22, 'preference': 23, 'cuisine': 24, 'which': 25, 'spanish': 26, 'i': 27, 'should': 28, 'let': 29, 'look': 30, '<SILENCE>': 31, 'today': 32, 'book': 33, 'your': 34, 'on': 35, 'can': 36, 'it': 37, 'many': 38, 'will': 39, 'are': 40, 'madrid': 41, 'help': 42, 'eight': 43, 'like': 44, 'bombay': 45, 'italian': 46, 'how': 47, 'any': 48, 'for': 49, 'with': 50, 'make': 51, 'api_call': 52, \"i'd\": 53, 'you': 54, 'hi': 55, 'looking': 56, 'expensive': 57, 'be': 58, 'hello': 59, 'love': 60, 'in': 61, 'party': 62, 'options': 63, 'table': 64, 'into': 65, 'where': 66, 'london': 67, 'two': 68, 'people': 69, 'restaurant': 70, 'rome': 71, 'cheap': 72, 'range': 73, 'type': 74, 'please': 75, 'british': 76, 'ok': 77}\n",
      "right now inside vectorize all\n",
      "length of train_context : 6024, length of train_response : 6024\n",
      "right now inside vectorize all\n",
      "right now inside vectorize all\n",
      "Candidate Size 4212\n",
      "train_file is data/train_data.txt \n",
      "validation file is data/val_data.txt \n",
      "test file is data/test_data.txt \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smajumdar/anaconda3/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 3713\n",
      "Longest sentence length 20\n",
      "Longest candidate sentence length 9\n",
      "Longest story length 14\n",
      "Average story length 5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-df0782ee7e28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m                                   data_dir='data/')\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_response\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-51-39f6937f2bcd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_context, train_response, train_candidates)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mtrain_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_make_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_file_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"length of train_context : {}, length of train_response : {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mtrain_candidates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_make_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_file_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/MODELS/supervised_embedding_model/make_tensor.py\u001b[0m in \u001b[0;36mnew_make_tensor\u001b[0;34m(train_filename, vocab)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorize_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/MODELS/supervised_embedding_model/make_tensor.py\u001b[0m in \u001b[0;36mload_train\u001b[0;34m(train_filename)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"opening file name : {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0msentence_pair\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_pair\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "vocab = load_vocab(vocab_file)\n",
    "print(\"Printing_Vocab\")\n",
    "print(\"type of vocab : {}\".format(type(vocab)))\n",
    "print(vocab)\n",
    "\n",
    "#train_context, train_response = new_make_tensor(train,vocab)\n",
    "print(\"length of train_context : {}, length of train_response : {}\".format(len(train_context),len(train_response)))\n",
    "#train_candidates, _ = new_make_tensor(candidates,vocab)\n",
    "\n",
    "#val_context,val_response = new_make_tensor(dev,vocab)\n",
    "\n",
    "model = SupervisedEmbeddingModel2(len(vocab),\n",
    "                                 embedding_size=embedding_size,\n",
    "                                 random_seed=42,\n",
    "                                 margin=margin,\n",
    "                                 negative_cand=negative_cand,\n",
    "                                 batch_size=batch_size,\n",
    "                                 epochs=epochs,\n",
    "                                  data_dir='data/')\n",
    "\n",
    "model.train(train_context,train_response,train_candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
