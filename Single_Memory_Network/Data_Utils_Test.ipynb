{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=set([\"a\",\"an\",\"the\",\":\",\",\",\"!\",\"!!\",\"!!!\",\":'\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sent):\n",
    "    '''Return the tokens of a sentence including punctuation.\n",
    "    >>> tokenize('Bob dropped the apple. Where is the apple?')\n",
    "    ['Bob', 'dropped', 'the', 'apple', '.', 'Where', 'is', 'the', 'apple']\n",
    "    '''\n",
    "    sent=sent.lower()\n",
    "    if sent=='<silence>':\n",
    "        return [sent]\n",
    "    result=[x.strip() for x in re.split('(\\W+)?', sent) if x.strip() and x.strip() not in stop_words]\n",
    "    if not result:\n",
    "        result=['<silence>']\n",
    "    if result[-1]=='.' or result[-1]=='?' or result[-1]=='!' or result[-1] == \"'\":\n",
    "        result=result[:-1]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"api_call:account_check_api, user_account:Credit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python35\\lib\\re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "tok_sentence = tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['api_call', 'account_check_api', 'user_account', 'credit']\n"
     ]
    }
   ],
   "source": [
    "print(tok_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_resp = \"api_response:account_check_api, api_result:failed, message:'available accounts : Savings,Credit'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_resp = tokenize(sentence_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['api_response', 'account_check_api', 'api_result', 'failed', 'message', 'available', 'accounts', 'savings', 'credit']\n"
     ]
    }
   ],
   "source": [
    "print(tok_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_tokenize(file_name_open,file_name_write) :\n",
    "    file_handle = open(file_name_open)\n",
    "    write_handle = open(file_name_write,'w')\n",
    "    for line in file_handle :\n",
    "        raw_line = line[2:].strip()\n",
    "        if not raw_line :\n",
    "            story = list()\n",
    "        else :\n",
    "            utterances = raw_line.split('\\t')\n",
    "            user_utterance = utterances[0]\n",
    "            bot_utterance = utterances[1]\n",
    "        \n",
    "            tok_user_utterance = tokenize(user_utterance)\n",
    "            tok_bot_utterance = tokenize(bot_utterance)\n",
    "        \n",
    "            write_handle.write('User : {}\\n'.format(tok_user_utterance))\n",
    "            write_handle.write('Bot : {}\\n\\n'.format(tok_bot_utterance))\n",
    "\n",
    "    write_handle.close()\n",
    "    file_handle.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_tokenize(file_name_open='../data/one_data/train_data.txt',file_name_write='tokenize_one_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_tokenize(file_name_open='../data/one_data_restaurant_domain/train_data.txt',file_name_write='tokenize_one_data_restaurant_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = [['api_call','initial_slot_check_api','user_account','savings','amount','2400']]\n",
    "candidate = ['api_call','initial_slot_check_api','user_account','savings']\n",
    "candidate2 = ['api_call','initial_slot_check_api','destination_name','Sourabh']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6065306597126334\n",
      "5.780789590099596e-155\n"
     ]
    }
   ],
   "source": [
    "print(sentence_bleu(reference,candidate))\n",
    "print(sentence_bleu(reference,candidate2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
