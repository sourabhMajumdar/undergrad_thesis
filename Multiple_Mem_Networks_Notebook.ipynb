{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python35\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "\n",
    "from data_utils import load_dialog_task, vectorize_data, load_candidates, vectorize_candidates, vectorize_candidates_sparse, tokenize\n",
    "from sklearn import metrics\n",
    "from memn2n import MemN2NDialog\n",
    "from itertools import chain\n",
    "from six.moves import range, reduce\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import win32com.client as wincl\n",
    "speak = wincl.Dispatch(\"SAPI.SpVoice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting pre defined variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "epsilon= 1e-8\n",
    "max_grad_norm = 40.0\n",
    "evaluation_interval = 10\n",
    "batch_size = 32\n",
    "hops = 3\n",
    "epochs = 20\n",
    "embedding_size = 20\n",
    "memory_size = 50\n",
    "task_id = 1\n",
    "random_state = None\n",
    "data_dir = \"data/dialog-bAbI-tasks/\"\n",
    "model_dir = \"model/\"\n",
    "train = True\n",
    "interactive = False\n",
    "OOV = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **chatBot** is the basic class that is implementing the *End-To-End Memory Network*.\n",
    "\n",
    "Each new instance of this class is a separate memory network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class chatBot(object):\n",
    "\n",
    "    def __init__(self, data_dir, model_dir, task_id, isInteractive, OOV, memory_size, random_state, batch_size, learning_rate, epsilon, max_grad_norm, evaluation_interval, hops, epochs, embedding_size,session,description):\n",
    "        self.data_dir = data_dir\n",
    "        self.task_id = task_id\n",
    "        self.model_dir = model_dir\n",
    "        # self.isTrain=isTrain\n",
    "        self.isInteractive = isInteractive\n",
    "        self.OOV = OOV\n",
    "        self.memory_size = memory_size\n",
    "        self.random_state = random_state\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.evaluation_interval = evaluation_interval\n",
    "        self.hops = hops\n",
    "        self.epochs = epochs\n",
    "        self.embedding_size = embedding_size\n",
    "        self.sess = session\n",
    "\n",
    "        self.description = description # set's the description of the Memory Network i.e which domain it deals wirh\n",
    "\n",
    "        candidates, self.candid2indx = load_candidates(\n",
    "            self.data_dir, self.task_id)\n",
    "        \n",
    "        self.n_cand = len(candidates)\n",
    "\n",
    "        self.indx2candid = dict(\n",
    "            (self.candid2indx[key], key) for key in self.candid2indx) # creates a reverse dictionary mapping an index to a candidate response\n",
    "        \n",
    "        # task data\n",
    "        self.trainData, self.testData, self.valData = load_dialog_task(\n",
    "            self.data_dir, self.task_id, self.candid2indx, self.OOV)\n",
    "        \n",
    "        \n",
    "        data = self.trainData + self.testData + self.valData\n",
    "        \n",
    "        # building vocabulary\n",
    "        self.build_vocab(data, candidates)\n",
    "        \n",
    "        #vectorize candidate\n",
    "        self.candidates_vec = vectorize_candidates(\n",
    "            candidates, self.word_idx, self.candidate_sentence_size)\n",
    "        \n",
    "        # create the optimizer\n",
    "        self.optimizer = tf.train.AdamOptimizer(\n",
    "            learning_rate=self.learning_rate, epsilon=self.epsilon)\n",
    "        \n",
    "\n",
    "        # create the actual memory End-To-End model\n",
    "        self.model = MemN2NDialog(self.batch_size, self.vocab_size, self.n_cand, self.sentence_size, self.embedding_size, self.candidates_vec, session=self.sess,\n",
    "                                  hops=self.hops, max_grad_norm=self.max_grad_norm, optimizer=self.optimizer, task_id=self.task_id)\n",
    "        \n",
    "        # stuff to save the memory network\n",
    "        self.saver = tf.train.Saver(max_to_keep=50)\n",
    "\n",
    "        self.summary_writer = tf.summary.FileWriter(\n",
    "            self.model.root_dir, self.model.graph_output.graph)\n",
    "\n",
    "\n",
    "    def build_vocab(self, data, candidates):\n",
    "        vocab = reduce(lambda x, y: x | y, (set(\n",
    "            list(chain.from_iterable(s)) + q) for s, q, a in data))\n",
    "        \n",
    "        vocab |= reduce(lambda x, y: x | y, (set(candidate)\n",
    "                                             for candidate in candidates))\n",
    "        \n",
    "        vocab = sorted(vocab)\n",
    "        \n",
    "        self.word_idx = dict((c, i + 1) for i, c in enumerate(vocab))\n",
    "        \n",
    "        max_story_size = max(map(len, (s for s, _, _ in data)))\n",
    "        \n",
    "        mean_story_size = int(np.mean([len(s) for s, _, _ in data]))\n",
    "        \n",
    "        self.sentence_size = max(\n",
    "            map(len, chain.from_iterable(s for s, _, _ in data)))\n",
    "        self.candidate_sentence_size = max(map(len, candidates))\n",
    "        \n",
    "        query_size = max(map(len, (q for _, q, _ in data)))\n",
    "        \n",
    "        self.memory_size = min(self.memory_size, max_story_size)\n",
    "        \n",
    "        self.vocab_size = len(self.word_idx) + 1  # +1 for nil word\n",
    "        \n",
    "        self.sentence_size = max(\n",
    "            query_size, self.sentence_size)  # for the position\n",
    "        \n",
    "        \n",
    "        # params\n",
    "        print(\"vocab size:\", self.vocab_size)\n",
    "        print(\"Longest sentence length\", self.sentence_size)\n",
    "        print(\"Longest candidate sentence length\",\n",
    "              self.candidate_sentence_size)\n",
    "        print(\"Longest story length\", max_story_size)\n",
    "        print(\"Average story length\", mean_story_size)\n",
    "\n",
    "    # The interaction interface between user and computer\n",
    "    def interactive(self):\n",
    "        \n",
    "        context = []\n",
    "        u = None\n",
    "        r = None\n",
    "        nid = 1\n",
    "        while True:\n",
    "            line = input('--> ').strip().lower()\n",
    "            if line == 'exit':\n",
    "                break\n",
    "            if line == 'restart':\n",
    "                context = []\n",
    "                nid = 1\n",
    "                print(\"clear memory\")\n",
    "                continue\n",
    "            u = tokenize(line)\n",
    "            data = [(context, u, -1)]\n",
    "            s, q, a = vectorize_data(\n",
    "                data, self.word_idx, self.sentence_size, self.batch_size, self.n_cand, self.memory_size)\n",
    "            preds = self.model.predict(s, q)\n",
    "            r = self.indx2candid[preds[0]]\n",
    "            print(r)\n",
    "            r = tokenize(r)\n",
    "            u.append('$u')\n",
    "            u.append('#' + str(nid))\n",
    "            r.append('$r')\n",
    "            r.append('#' + str(nid))\n",
    "            context.append(u)\n",
    "            context.append(r)\n",
    "            nid += 1\n",
    "\n",
    "    # The training function\n",
    "    def train(self):\n",
    "        trainS, trainQ, trainA = vectorize_data(\n",
    "            self.trainData, self.word_idx, self.sentence_size, self.batch_size, self.n_cand, self.memory_size)\n",
    "        \n",
    "        valS, valQ, valA = vectorize_data(\n",
    "            self.valData, self.word_idx, self.sentence_size, self.batch_size, self.n_cand, self.memory_size)\n",
    "        \n",
    "        n_train = len(trainS)\n",
    "        n_val = len(valS)\n",
    "        \n",
    "        print(\"Training Size\", n_train)\n",
    "        print(\"Validation Size\", n_val)\n",
    "        \n",
    "        tf.set_random_seed(self.random_state)\n",
    "        \n",
    "        batches = zip(range(0, n_train - self.batch_size, self.batch_size),\n",
    "                      range(self.batch_size, n_train, self.batch_size))\n",
    "        \n",
    "        batches = [(start, end) for start, end in batches]\n",
    "        \n",
    "        best_validation_accuracy = 0\n",
    "\n",
    "        for t in range(1, self.epochs + 1):\n",
    "            \n",
    "            np.random.shuffle(batches)\n",
    "            \n",
    "            total_cost = 0.0\n",
    "            \n",
    "            for start, end in batches:\n",
    "                \n",
    "                s = trainS[start:end]\n",
    "                q = trainQ[start:end]\n",
    "                a = trainA[start:end]\n",
    "                \n",
    "                cost_t = self.model.batch_fit(s, q, a)\n",
    "                total_cost += cost_t\n",
    "            \n",
    "            if t % self.evaluation_interval == 0:\n",
    "                \n",
    "                train_preds = self.batch_predict(trainS, trainQ, n_train)\n",
    "                val_preds = self.batch_predict(valS, valQ, n_val)\n",
    "                train_acc = metrics.accuracy_score(\n",
    "                    np.array(train_preds), trainA)\n",
    "                val_acc = metrics.accuracy_score(val_preds, valA)\n",
    "                print('-----------------------')\n",
    "                print('Epoch', t)\n",
    "                print('Total Cost:', total_cost)\n",
    "                print('Training Accuracy:', train_acc)\n",
    "                print('Validation Accuracy:', val_acc)\n",
    "                print('-----------------------')\n",
    "\n",
    "                # write summary\n",
    "                \n",
    "                train_acc_summary = tf.summary.scalar(\n",
    "                    'task_' + str(self.task_id) + '/' + 'train_acc', tf.constant((train_acc), dtype=tf.float32))\n",
    "                \n",
    "                val_acc_summary = tf.summary.scalar(\n",
    "                    'task_' + str(self.task_id) + '/' + 'val_acc', tf.constant((val_acc), dtype=tf.float32))\n",
    "                \n",
    "                merged_summary = tf.summary.merge(\n",
    "                    [train_acc_summary, val_acc_summary])\n",
    "                \n",
    "                summary_str = self.sess.run(merged_summary)\n",
    "                \n",
    "                self.summary_writer.add_summary(summary_str, t)\n",
    "                self.summary_writer.flush()\n",
    "\n",
    "                if val_acc > best_validation_accuracy:\n",
    "                    \n",
    "                    best_validation_accuracy = val_acc\n",
    "                    self.saver.save(self.sess, self.model_dir +\n",
    "                                    'model.ckpt', global_step=t)\n",
    "\n",
    "    def converse(self) :\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(self.model_dir)\n",
    "        \n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            print(\"...no checkpoint found...\")\n",
    "        \n",
    "        context = []\n",
    "        u = None\n",
    "        r = None\n",
    "        nid = 1\n",
    "        \n",
    "        while True:\n",
    "            \n",
    "            line = input('--> ').strip().lower()\n",
    "            if line == 'exit':\n",
    "                break\n",
    "            if line == 'restart':\n",
    "                context = []\n",
    "                nid = 1\n",
    "                print(\"clear memory\")\n",
    "                continue\n",
    "            \n",
    "            u = tokenize(line)\n",
    "            data = [(context, u, -1)]\n",
    "            \n",
    "            s, q, a = vectorize_data(\n",
    "                data, self.word_idx, self.sentence_size, self.batch_size, self.n_cand, self.memory_size)\n",
    "            \n",
    "            preds = self.model.predict(s, q)\n",
    "            \n",
    "            r = self.indx2candid[preds[0]]\n",
    "            \n",
    "            print(r)\n",
    "            \n",
    "            r = tokenize(r)\n",
    "            u.append('$u')\n",
    "            u.append('#' + str(nid))\n",
    "            r.append('$r')\n",
    "            r.append('#' + str(nid))\n",
    "            context.append(u)\n",
    "            context.append(r)\n",
    "            nid += 1\n",
    "\n",
    "    def converse_2(self,context,user_utterance,nid) :\n",
    "\n",
    "        print(\" hi from {} memory network \".format(self.description))\n",
    "\n",
    "        ckpt = tf.train.get_checkpoint_state(self.model_dir)\n",
    "        \n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            print(\"...no checkpoint found...\")\n",
    "\n",
    "        u = None\n",
    "        r = None\n",
    "\n",
    "        line = user_utterance.strip().lower()\n",
    "        \n",
    "        u = tokenize(line)\n",
    "        data = [(context, u, -1)]\n",
    "        \n",
    "        s, q, a = vectorize_data(\n",
    "            data, self.word_idx, self.sentence_size, self.batch_size, self.n_cand, self.memory_size)\n",
    "        \n",
    "        preds = self.model.predict(s, q)\n",
    "        \n",
    "        r = self.indx2candid[preds[0]]\n",
    "        bot_response = r\n",
    "        print(r)\n",
    "        \n",
    "        r = tokenize(r)\n",
    "        u.append('$u')\n",
    "        u.append('#' + str(nid))\n",
    "        r.append('$r')\n",
    "        r.append('#' + str(nid))\n",
    "        context.append(u)\n",
    "        context.append(r)\n",
    "        nid += 1\n",
    "        \n",
    "        return context, bot_response, nid\n",
    "\n",
    "\n",
    "    def test(self):\n",
    "        \n",
    "        print(\" Hey I am in the testing phase, model_dir is : \",self.model_dir)\n",
    "        berger = input()\n",
    "        \n",
    "        ckpt = tf.train.get_checkpoint_state(self.model_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess, ckpt.model_checkpoint_path)\n",
    "        else:\n",
    "            print(\"...no checkpoint found...\")\n",
    "        if self.isInteractive:\n",
    "            self.interactive()\n",
    "        else:\n",
    "            testS, testQ, testA = vectorize_data(\n",
    "                self.testData, self.word_idx, self.sentence_size, self.batch_size, self.n_cand, self.memory_size)\n",
    "            n_test = len(testS)\n",
    "            print(\"Testing Size\", n_test)\n",
    "            test_preds = self.batch_predict(testS, testQ, n_test)\n",
    "            test_acc = metrics.accuracy_score(test_preds, testA)\n",
    "            print(\"Testing Accuracy:\", test_acc)\n",
    "\n",
    "    def batch_predict(self, S, Q, n):\n",
    "        preds = []\n",
    "        for start in range(0, n, self.batch_size):\n",
    "            end = start + self.batch_size\n",
    "            s = S[start:end]\n",
    "            q = Q[start:end]\n",
    "            pred = self.model.predict(s, q)\n",
    "            preds += list(pred)\n",
    "        return preds\n",
    "\n",
    "    def close_session(self):\n",
    "        self.sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"main_mem_network/\"):\n",
    "    os.makedirs(\"main_mem_network\")\n",
    "\n",
    "if not os.path.exists(\"loan_mem_network/\"):\n",
    "    os.makedirs(\"loan_mem_network\")\n",
    "\n",
    "if not os.path.exists(\"balance_mem_network/\"):\n",
    "    os.makedirs(\"balance_mem_network\")\n",
    "\n",
    "if not os.path.exists(\"transaction_mem_network/\"):\n",
    "    os.makedirs(\"transaction_mem_network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g1 = tf.Graph()\n",
    "g2 = tf.Graph()\n",
    "g3 = tf.Graph()\n",
    "g4 = tf.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_session = tf.Session(graph=g1)\n",
    "loan_session = tf.Session(graph=g2)\n",
    "balance_session = tf.Session(graph=g3)\n",
    "transaction_session = tf.Session(graph=g4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python35\\lib\\re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 3715\n",
      "Longest sentence length 21\n",
      "Longest candidate sentence length 9\n",
      "Longest story length 14\n",
      "Average story length 5\n",
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "WARNING:tensorflow:From c:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "main_mem_network created successfully !!\n",
      "vocab size: 3715\n",
      "Longest sentence length 21\n",
      "Longest candidate sentence length 9\n",
      "Longest story length 14\n",
      "Average story length 5\n",
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "loan_mem_network created successfully !!\n",
      "vocab size: 3715\n",
      "Longest sentence length 21\n",
      "Longest candidate sentence length 9\n",
      "Longest story length 14\n",
      "Average story length 5\n",
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "balance_mem_network created successfully !!\n",
      "vocab size: 3715\n",
      "Longest sentence length 21\n",
      "Longest candidate sentence length 9\n",
      "Longest story length 14\n",
      "Average story length 5\n",
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "WARNING:tensorflow:tf.op_scope(values, name, default_name) is deprecated, use tf.name_scope(name, default_name, values)\n",
      "transaction_mem_network created successfully !!\n"
     ]
    }
   ],
   "source": [
    "with g1.as_default() :\n",
    "    main_mem_network = chatBot(data_dir=data_dir, \n",
    "                               model_dir=\"main_mem_network/\",   \n",
    "                               task_id=1,  \n",
    "                               isInteractive=interactive,    \n",
    "                               OOV=OOV,\n",
    "                               memory_size=memory_size,\n",
    "                               random_state=random_state,\n",
    "                               batch_size=batch_size,\n",
    "                               learning_rate=learning_rate,\n",
    "                               epsilon=epsilon,\n",
    "                               max_grad_norm=max_grad_norm,\n",
    "                               evaluation_interval=evaluation_interval,\n",
    "                               hops=hops,\n",
    "                               epochs=epochs,\n",
    "                               embedding_size=embedding_size,\n",
    "                               session=main_session,\n",
    "                               description=\"main\")\n",
    "\n",
    "\n",
    "main = input(\"main_mem_network created successfully !!\")\n",
    "\n",
    "with g2.as_default() :\n",
    "    loan_mem_network = chatBot(data_dir=data_dir, \n",
    "                               model_dir=\"loan_mem_network/\",    \n",
    "                               task_id=1,  \n",
    "                               isInteractive=interactive,    \n",
    "                               OOV=OOV,\n",
    "                               memory_size=memory_size,\n",
    "                               random_state=random_state,\n",
    "                               batch_size=batch_size,\n",
    "                               learning_rate=learning_rate,\n",
    "                               epsilon=epsilon,\n",
    "                               max_grad_norm=max_grad_norm,\n",
    "                               evaluation_interval=evaluation_interval,\n",
    "                               hops=hops,\n",
    "                               epochs=epochs,\n",
    "                               embedding_size=embedding_size,\n",
    "                               session=loan_session,\n",
    "                               description=\"loan\")\n",
    "\n",
    "loan = input(\"loan_mem_network created successfully !!\")\n",
    "    \n",
    "with g3.as_default() :\n",
    "    balance_mem_network = chatBot(data_dir=data_dir, \n",
    "                                  model_dir=\"balance_mem_network/\",\n",
    "                                  task_id=1,  \n",
    "                                  isInteractive=interactive,    \n",
    "                                  OOV=OOV,\n",
    "                                  memory_size=memory_size,\n",
    "                                  random_state=random_state,\n",
    "                                  batch_size=batch_size,\n",
    "                                  learning_rate=learning_rate,\n",
    "                                  epsilon=epsilon,\n",
    "                                  max_grad_norm=max_grad_norm,\n",
    "                                  evaluation_interval=evaluation_interval,\n",
    "                                  hops=hops,\n",
    "                                  epochs=epochs,\n",
    "                                  embedding_size=embedding_size,\n",
    "                                  session=balance_session,\n",
    "                                  description=\"balance\")\n",
    "    \n",
    "balance = input(\"balance_mem_network created successfully !!\")\n",
    "    \n",
    "with g4.as_default() :\n",
    "    transaction_mem_network = chatBot(data_dir=data_dir, \n",
    "                                      model_dir=\"transaction_mem_network/\",\n",
    "                                      task_id=task_id,\n",
    "                                      isInteractive=interactive,    \n",
    "                                      OOV=OOV,\n",
    "                                      memory_size=memory_size,\n",
    "                                      random_state=random_state,\n",
    "                                      batch_size=batch_size,\n",
    "                                      learning_rate=learning_rate,\n",
    "                                      epsilon=epsilon,\n",
    "                                      max_grad_norm=max_grad_norm,\n",
    "                                      evaluation_interval=evaluation_interval,\n",
    "                                      hops=hops,\n",
    "                                      epochs=epochs,\n",
    "                                      embedding_size=embedding_size,\n",
    "                                      session=transaction_session,\n",
    "                                      description=\"transaction\")\n",
    "\n",
    "transaction = input(\"transaction_mem_network created successfully !!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size 6024\n",
      "Validation Size 6015\n",
      "-----------------------\n",
      "Epoch 10\n",
      "Total Cost: 6733.381484591868\n",
      "Training Accuracy: 0.7719123505976095\n",
      "Validation Accuracy: 0.7664172901080631\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 20\n",
      "Total Cost: 4843.4042185392755\n",
      "Training Accuracy: 0.8366533864541833\n",
      "Validation Accuracy: 0.828761429758936\n",
      "-----------------------\n",
      "Training Size 6024\n",
      "Validation Size 6015\n",
      "-----------------------\n",
      "Epoch 10\n",
      "Total Cost: 5078.489626877825\n",
      "Training Accuracy: 0.7866865869853917\n",
      "Validation Accuracy: 0.7650872817955112\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 20\n",
      "Total Cost: 2505.8804769558683\n",
      "Training Accuracy: 0.8962483399734396\n",
      "Validation Accuracy: 0.857024106400665\n",
      "-----------------------\n",
      "Training Size 6024\n",
      "Validation Size 6015\n",
      "-----------------------\n",
      "Epoch 10\n",
      "Total Cost: 4919.041832503863\n",
      "Training Accuracy: 0.8359893758300133\n",
      "Validation Accuracy: 0.8252701579384871\n",
      "-----------------------\n",
      "-----------------------\n",
      "Epoch 20\n",
      "Total Cost: 2773.5158464015803\n",
      "Training Accuracy: 0.9023904382470119\n",
      "Validation Accuracy: 0.8739817123857024\n",
      "-----------------------\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Failed to rename: balance_mem_network\\model.ckpt-20.meta.tmpc97d1c3a75984133b760670bb078b8f2 to: balance_mem_network\\model.ckpt-20.meta : The process cannot access the file because it is being used by another process.\r\n; Broken pipe",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-f50b6168a4ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mloan_mem_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mg3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mbalance_mem_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mg4\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mtransaction_mem_network\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-a1d8227be3bc>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    201\u001b[0m                     \u001b[0mbest_validation_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m                     self.saver.save(self.sess, self.model_dir +\n\u001b[1;32m--> 203\u001b[1;33m                                     'model.ckpt', global_step=t)\n\u001b[0m\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mconverse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state, strip_default_attrs)\u001b[0m\n\u001b[0;32m   1643\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1644\u001b[0m           self.export_meta_graph(\n\u001b[1;32m-> 1645\u001b[1;33m               meta_graph_filename, strip_default_attrs=strip_default_attrs)\n\u001b[0m\u001b[0;32m   1646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_empty\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[1;34m(self, filename, collection_list, as_text, export_scope, clear_devices, clear_extraneous_savers, strip_default_attrs)\u001b[0m\n\u001b[0;32m   1688\u001b[0m         \u001b[0mclear_devices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclear_devices\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1689\u001b[0m         \u001b[0mclear_extraneous_savers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclear_extraneous_savers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1690\u001b[1;33m         strip_default_attrs=strip_default_attrs)\n\u001b[0m\u001b[0;32m   1691\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1692\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mrestore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\training\\saver.py\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[1;34m(filename, meta_info_def, graph_def, saver_def, collection_list, as_text, graph, export_scope, clear_devices, clear_extraneous_savers, strip_default_attrs, **kwargs)\u001b[0m\n\u001b[0;32m   2033\u001b[0m       \u001b[0mclear_extraneous_savers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclear_extraneous_savers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2034\u001b[0m       \u001b[0mstrip_default_attrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrip_default_attrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2035\u001b[1;33m       **kwargs)\n\u001b[0m\u001b[0;32m   2036\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2037\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\meta_graph.py\u001b[0m in \u001b[0;36mexport_scoped_meta_graph\u001b[1;34m(filename, graph_def, graph, export_scope, as_text, unbound_inputs_col_name, clear_devices, saver_def, clear_extraneous_savers, strip_default_attrs, **kwargs)\u001b[0m\n\u001b[0;32m    943\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 945\u001b[1;33m         as_text=as_text)\n\u001b[0m\u001b[0;32m    946\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mscoped_meta_graph_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\graph_io.py\u001b[0m in \u001b[0;36mwrite_graph\u001b[1;34m(graph_or_graph_def, logdir, name, as_text)\u001b[0m\n\u001b[0;32m     71\u001b[0m                                         text_format.MessageToString(graph_def))\n\u001b[0;32m     72\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     \u001b[0mfile_io\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matomic_write_string_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSerializeToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36matomic_write_string_to_file\u001b[1;34m(filename, contents, overwrite)\u001b[0m\n\u001b[0;32m    434\u001b[0m   \u001b[0mwrite_string_to_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_pathname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 436\u001b[1;33m     \u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_pathname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    437\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[0mdelete_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_pathname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\lib\\io\\file_io.py\u001b[0m in \u001b[0;36mrename\u001b[1;34m(oldname, newname, overwrite)\u001b[0m\n\u001b[0;32m    413\u001b[0m   \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m     pywrap_tensorflow.RenameFile(\n\u001b[1;32m--> 415\u001b[1;33m         compat.as_bytes(oldname), compat.as_bytes(newname), overwrite, status)\n\u001b[0m\u001b[0;32m    416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python35\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    517\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    520\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Failed to rename: balance_mem_network\\model.ckpt-20.meta.tmpc97d1c3a75984133b760670bb078b8f2 to: balance_mem_network\\model.ckpt-20.meta : The process cannot access the file because it is being used by another process.\r\n; Broken pipe"
     ]
    }
   ],
   "source": [
    "if train:\n",
    "    with g1.as_default() :\n",
    "        main_mem_network.train()\n",
    "    with g2.as_default() :\n",
    "        loan_mem_network.train()\n",
    "    with g3.as_default():\n",
    "        balance_mem_network.train()\n",
    "    with g4.as_default() :\n",
    "        transaction_mem_network.train()\n",
    "else:\n",
    "    with g1.as_default() :\n",
    "        main_mem_network.test()\n",
    "    with g2.as_default() :\n",
    "        loan_mem_network.test()\n",
    "    with g3.as_default():\n",
    "        balance_mem_network.test()\n",
    "    with g4.as_default() :\n",
    "        transaction_mem_network.test()\n",
    "    #chatbot.converse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "network_dict = {\"main\" : [main_mem_network.converse_2,g1] , \"loan\" : [loan_mem_network.converse_2,g2], \"balance\" : [balance_mem_network.converse_2,g3], \"transaction\" : [transaction_mem_network.converse_2,g4]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >>> : hello\n",
      " hi from main memory network \n",
      "INFO:tensorflow:Restoring parameters from main_mem_network/model.ckpt-20\n",
      "hello what can i help you with today\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python35\\lib\\re.py:203: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >>> : i am looking for a restaurant for two people\n",
      " hi from main memory network \n",
      "INFO:tensorflow:Restoring parameters from main_mem_network/model.ckpt-20\n",
      "any preference on a type of cuisine\n",
      " >>> : italian please\n",
      " hi from loan memory network \n",
      "INFO:tensorflow:Restoring parameters from loan_mem_network/model.ckpt-20\n",
      "ok let me look into some options for you\n",
      " >>> : <SILENCE>\n",
      " hi from loan memory network \n",
      "INFO:tensorflow:Restoring parameters from loan_mem_network/model.ckpt-20\n",
      "api_call italian paris two cheap\n",
      " >>> : restart\n",
      " >>> : hello\n",
      " hi from balance memory network \n",
      "INFO:tensorflow:Restoring parameters from balance_mem_network/model.ckpt-20\n",
      "hello what can i help you with today\n",
      " >>> : i am looking for a restaurant for eight people\n",
      " hi from balance memory network \n",
      "INFO:tensorflow:Restoring parameters from balance_mem_network/model.ckpt-20\n",
      "api_call french london four expensive\n"
     ]
    }
   ],
   "source": [
    "restart = False\n",
    "exit = False\n",
    "\n",
    "story = list()\n",
    "nid = 1\n",
    "bot_utterance = None\n",
    "network_converse = main_mem_network.converse_2\n",
    "network_graph = g1\n",
    "while True :\n",
    "    with network_graph.as_default() :\n",
    "        user_utterance = input(\" >>> : \")\n",
    "        if user_utterance == \"exit\" :\n",
    "            exit = True\n",
    "            break\n",
    "        if user_utterance == \"restart\" :\n",
    "            story = list()\n",
    "            nid = 1\n",
    "            network_graph = g1\n",
    "            continue\n",
    "        story, bot_utterance, nid = network_converse(story,user_utterance,nid)\n",
    "        speak.Speak(bot_utterance)\n",
    "        if nid%3 == 0 :\n",
    "            network_converse = network_dict[\"loan\"][0]\n",
    "            network_graph = network_dict[\"loan\"][1]\n",
    "        if nid%5 == 0 :\n",
    "            network_converse = network_dict[\"balance\"][0]\n",
    "            network_graph = network_dict[\"balance\"][1]\n",
    "        if nid%7 == 0 :\n",
    "            network_converse = network_dict[\"transaction\"][0]\n",
    "            network_graph = network_dict[\"transaction\"][1]\n",
    "        if nid%9 == 0 :\n",
    "            network_converse = network_dict[\"main\"][0]\n",
    "            network_graph = network_dict[\"main\"][1]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "main_mem_network.close_session()\n",
    "loan_mem_network.close_session()\n",
    "balance_mem_network.close_session()\n",
    "transaction_mem_network.close_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
